---
layout: post
title: MLAPP-01-Introduction
toc: true
tag:
  - MLAPP
  - 翻译
---

## 1.1 什么是机器学习, 为啥要它?

> We are drowning in information and starving for knowledge. -- John Naisbitt

我们正在进入一个大数据时代. 网页上10亿; 油管视频暴涨; 人类基因丰富; 沃尔玛每秒交易量上百万, 其数据库数据量达P级($$10^15$$)之多.

大数据需要依靠机器学习来进行自动化数据分析. 具体地, 机器学习指的是, 自动地对数据进行分析, 发现规律, 并以此预测未来的数据, 或者依靠规律对未来进行决策.

本书认为, 概率论是解决此问题的最佳方案. 概率论能够在含有不确定性的问题上发挥作用. 在机器学习中, 不确定性可能是: 在以往的数据的基础上, 最佳的未来预测是什么? 对数据最佳的刻画模型是什么? 我在下一步的最佳策略是什么? 等等. 机器学习相关的概率论和统计理论关联密切, 但是在某些表述和侧重点有所不同.

我们将阐述大量的概率模型, 这些模型适用于不同的数据和任务. 同时, 我们也将阐明大量的算法用于训练和使用这些模型. 本书并非特定问题的参考答案, 而是要为读者提供一个综合的视角来研究概率模型的建模和应用. 我们同样会涉及计算效率的问题, 针对如何将方法扩展到真实的大规模数据的现实场景中, 在其它书籍中会得到更具体地阐述, 这方面书籍有(Rajaraman and Ullman 2011[^1]; Bekkerman et al. 2011[^2])

[^1]: Mining of massive datasets.
[^2]: Scaling up maching learning

值得注意的是, 大量的数据中, 其实真正有效的实际上只有很小一部分. 事实上, 很多领域的数据都具有"长尾"的现象, **长尾**体现在, 极小部分(指种类)的数据是非常常见的, 而大部分数据则是非常少见的. 例如, 谷歌每天有20%的搜索都是过去未见的. 这就意味着本书讨论的核心的关于如何从小规模数据中进行泛化的统计问题, 在大数据背景下, 仍然是非常有用的.

### 1.1.1 机器学习的类型

机器学习主要被分为两种, 其中一种为预测型(**predictive**)或者有监督学习(**supervised learning**). 有监督学习的目标是在给定输入$$\mathbf{x}$$输出$$y$$对集合, $$\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i}^{N}$$, 学习从输入到输出的映射. 这个集合 $$\mathcal{D}$$称为训练集, 而 $$N$$就是样本数.

举个简单的例子, 每个样本的输入 $$\mathbf{x}_i$$ 是一个 $$D$$维的向量, 例如代表身材的身高和体重. 这个向量称为 **特征**, **属性**或者 **协变量**. 一般, 输入$$\mathcal{x}_i$$可以是复杂的数据结构对象, 例如图像, 句子, 电子邮件, 时间序列, 分子, 图等等.

类似的, 输出或者称为**响应值**可以是任何形式的数据, 但在大多数方法, $$y_i$$是一个**类别**或者属于某个有限集合的**代表**, 例如用$$y_i \in \{1, ... C\}$$ 代表男性, 女性; 或者是浮点数$$\y_i$$, 代表收入. 当 $$y_i$$ 是类别的是, 问题被称为分类, 或者是模式识别, 当 $$y_i$$ 是个浮点数时, 问题则是回归问题. 另一类问题, 被称为有序回归, 这类问题中, 输出空间 $$\mathcal{Y}$$ 有序, 想评价等级 A-F.

另一种类型的机器学习称为**描述性**或者**无监督学习**. 在这类问题中, 我们只有输入集合 $$\mathcal{D} = \{(\mathbf{x}_i\}_{i}^{N}$$, 目标是找到数据中隐含的模式. 这类问题有时候被称为**知识挖掘**(**knowledge discovery**). 目前这类问题, 缺乏全面的定义, 毕竟我们并不知道我们要寻找什么样的模式. 而且没有明确评价指标来估计误差(不想监督学习, 可以直接对比模型预测与真实的标签).

机器学习其实还有第三种, 称为**强化学习**. 该类方法主要为了解决, 在奖惩环境下如何行动. (就像婴儿学步). 虽然我们将在第5.7节中讨论决策论, 但是强化学习不在本书的讨论范围. 

## 1.2 监督学习

下面开始讨论机器学习中运用最为广泛的监督学习.

### 1.2.1 分类

本节, 我们讨论分类问题. 分类问题的目标是学习输入 $$\mathbf{x}$$ 到输出 $$y$$ 的映射, 其中 $$y\in \{1, \cdots, C\}$$, $$C$$是类别总数. 如果 $$C = 2$$ 则问题是**二分类**问题(此时 $$y\in \{0, 1\}$$); 如果 $$C > 2$$ 则问题是**多分类**问题. 如果各个类别不是互斥的(例如, 某个人可以同时被分类为高大的, 和强壮的), 此时这类问题称之为**多标签分类**问题, 但是这类问题更恰当地应当看做多个相关的二分类问题(即通常说的**多输出模型**). 当我们提到**分类问题**时, 默认多分类问题采用单个输出方式, 除非另有说明.

分类问题通常被表述为**函数逼近**问题. 学习过程是用带标签的训练集估计函数 $$f$$ 概函数有 $$y = f(x)$$, 然后用这个估计得的函数 $$\hat{f}$$(戴帽子表示估计得函数) 对新的的输入中进行类别预测 $$\hat{y} = \hat{f} x) $$. 对新的输入的预测能力称之为**范化能力**.

#### 1.2.1.1 例子

我们来看一个简单的例子, 如 Figure1.1(a) 所示. 有两种类型的物体, 分别标签为 0 和 1. 输入为彩色的图形. 这些图像可以用 $$D$$ 维的特征或者属性, 这些特征用 $$N\times D$$ 的矩阵表示, 如 Figure1.1(b) 所示. 输入特征 $$\mathbf{x}$$ 可以是离散, 连续的或者二者兼而有之. 除此之外, 我们还有对应的输出标签 $$y$$.

![](/assets/img/MLAPP-Figure-1.1.png){:height="300px"}

在图 Figure1.1 中, 测试样本是一个蓝色的月牙, 一个黄色圆环和一个蓝色的箭头. 这些形状在训练样本中都未曾见过. 我们需要用训练集**范化**到这些未曾见过的样本. 我们有理由猜测, 蓝色的月牙的标签应该为 $$y=1$$, 因为训练集中所有的蓝色的图形标签都为 1. 黄色的圆环则更难分类, 因为训练集中黄色的图形部分标签为 $$y=0$$, 部分为 $$y=0$$, 圆形图案也有两种标签. 因此, 黄色的圆环的标签不明确. 类似地, 蓝色的箭头的标签也不明确.

#### 1.2.1.2 概率预测的必要性

为了解决上述的模棱两可的困境, 我们需要预测概率. 本书假设读者掌握一定的概率论概念. 如果没有可以参看第二章的内容. 

给定训练集 $$\mathcal{D}$$ 和 输入向量 $$\mathbf{x}$$ 所有可能的标签的概率表示为 $$p(y \mid \mathbf{x}, \mathcal{D})$$. 这实际上是一个长度为 $$C$$ 的向量, 其中 $$C$$ 是类别数. 对于二分类问题, 直接给出一个数 $$p(y=1 \mid \mathbf{x}, \mathcal{D}$$ 就足够了, 因为 $$p(y=1 \mid \mathbf{x}, \mathcal{D}) + p(y=0 \mid \mathbf{x}, \mathcal{D}) = 1$$. 从表达式可见, 概率是训练集和输入向量的条件概率. 当模型参与预测是, 我们还可以显示地将概率表示为所使用的模型的条件概率: $$p(y \mid \mathbf{x}, {M}, \mathcal{D})$$, 但是如果上下文明显表明包含模型, 我们会将 $$M$$ 省略.

有了概率, 我们可以计算概率最大的标签作为"最佳估计", 表达式如下:

$$
  \hat{y} = \hat{f}(x) = \overset{C}{\underset{c=1}{\operatorname{argmax}}} \ p(y=c| \mathbf{x}, \mathcal{D}) \tag{1.1}
$$

这就是可能性最大的标签, 称之为分布 $$p(y \mid \mathbf{x}, \mathcal{D})$$ 的**众数**(**mode**); 这也就是**最大后验概率估计**(**M**aximum **A** **P**osteriori estimation). 使用概率最大的标签作为结果在直觉上是合理的, 5.7节中将进行更正式的阐述.

现在假设这样的情形, 黄色的圆环的概率 $$p(\hat{y} \mid \mathbf{x}, \mathcal{D})$$ 远低于 1.0. 也就是说, 我们没有十足的把我给出我们的答案, 此时可能我们直接说"我不知道"比给一个我们自己也不确信的答案来的更好一些. 这些情况在一些医学和金融场景尤为重要, 毕竟在这些应用场景中, 我们不值得冒险, 这些将在 5.7 章中进行解释. 还有其它的应用场景评估风险也非常重要, 毕竟这些场景中可能稍有不慎会让你输钱.

#### 1.2.1.3 真实案例

分类问题应用广泛, 解决了许多有趣的, 高难度的问题. 下面举一些现实例子.

**文档分类和垃圾邮件过滤**

**文档分类**(**document classification**) 目标是将文档, 如网页, 邮件等归类. 垃圾邮件过滤是其中一个特例, 是一个文档二分类问题, 将邮件分为是垃圾邮件 $$y = 1$$ 和 不是 $$y=0$$.

![](/assets/img/MLAPP-Figure1.2.png){:height="480px"}


